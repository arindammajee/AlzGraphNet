{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6548c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from dataLoader import *\n",
    "from sklearn import neighbors\n",
    "import time\n",
    "from prettytable import PrettyTable\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a5844f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images are: 708\n",
      "\n",
      "Shape inconsistancy found! (256, 256, 123) for /home/arindam/Alzheimer/Data/MIRIAD/miriad/miriad_192_AD_M/miriad_192_AD_M_06_MR_2/miriad_192_AD_M_06_MR_2.nii\n",
      "\n",
      "\n",
      "Remove shape insconsitent images.\n",
      "After Removing shape inconsistent images total number of images 707.\n",
      "Number of Alzheimer infected MRI scans: 464\n",
      "Number of Healthy MRI scans: 243\n",
      "Everything is fine. No of images in train, val and test set is 371, 46 and 47 respectively.\n",
      "Everything is fine. No of images in train, val and test set is 194, 24 and 25 respectively.\n",
      "Total no of train, validation and test images are 565, 70 and 72 respectively.\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "DATA_PATH = os.path.join('/home/arindam/Alzheimer', 'Data/MIRIAD/miriad')\n",
    "config = {\n",
    "    'img_size': 256,\n",
    "    'depth' : 64,\n",
    "    'batch_size' : 8\n",
    "}\n",
    "\n",
    "# Modify the above config in the Dataloader to change the batch size, image size and depth of the model\n",
    "train_loader, valid_loader, test_loader = LoadDatasets(return_type='loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f16865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the kernerl size, stride and number of features\n",
    "\n",
    "kc, kh, kw = 16, 16, 16  # kernel size\n",
    "dc, dh, dw = 16, 16, 16  # stride\n",
    "num_features = kc*kh*kw\n",
    "num_classes = 2\n",
    "num_of_patch_in_each_image = config['img_size']//16 * config['img_size']//16 * config['depth']//16\n",
    "\n",
    "\n",
    "# Build the graph using KNN. The graph is built on the patches of the images\n",
    "\n",
    "def build_graph(batched_images):\n",
    "    batch, patch_batch = [], []\n",
    "    for img in batched_images:\n",
    "        patches = img.unfold(1, kc, dc).unfold(2, kh, dh).unfold(3, kw, dw)\n",
    "        patches = patches.contiguous().view(patches.size(0), -1, kc, kh, kw)\n",
    "        patch_batch.append(patches)\n",
    "        patches = patches.contiguous().view(patches.size(0), -1, kc*kh*kw)\n",
    "        batch.append(patches)\n",
    "        \n",
    "    patched_images = torch.cat(batch, dim=0)  # Shape -> (batch_size, num_of_patch_in_each_image, num_features)\n",
    "    patch_batch = torch.cat(patch_batch, dim=0)  # Shape -> (batch_size, num_of_patch_in_each_image, kc, kh, kw) This is required for local 3D CNNs\n",
    "    batch_adj = []\n",
    "    for i in range(patched_images.shape[0]):\n",
    "        patches = patched_images[i]\n",
    "        adj = torch.as_tensor(neighbors.kneighbors_graph(patches, n_neighbors = 64).toarray(), dtype=torch.float32)  # No of neighbors = 64\n",
    "        adj = adj.reshape(1, adj.shape[0], adj.shape[1])\n",
    "        batch_adj.append(adj)\n",
    "        \n",
    "    adj = torch.cat(batch_adj, dim=0)\n",
    "    return patch_batch.type(torch.FloatTensor), adj\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d147f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 16, 16, 16]) torch.Size([8, 1024, 1024]) torch.Size([8, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Check a sample batch size\n",
    "for data in train_loader:\n",
    "    images, labels = data\n",
    "    patched_images, adj = build_graph(images)  \n",
    "    print(patched_images.shape, adj.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f07b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Base paper: https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_node_features, num_classes, hidden_channels, linear_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        # 3D Convolutional layer\n",
    "        self.conv3d = nn.Conv3d(1, 16, 3, stride=1, padding=1)\n",
    "        self.maxpool = nn.MaxPool3d(3, stride=2, padding=1)\n",
    "        self.conv3d_2 = nn.Conv3d(16, 32, 3, stride=2, padding=1)\n",
    "        self.maxpool_2 = nn.MaxPool3d(3, stride=2, padding=1)\n",
    "        \n",
    "        # Graph convolution layer\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels//2)\n",
    "        self.conv3 = GCNConv(hidden_channels//2, hidden_channels//4)\n",
    "        self.maxpool1d = nn.MaxPool1d(64) # 32 is the kernel size\n",
    "        self.avgpool1d = nn.AvgPool1d(64) # 32 is the kernel size\n",
    "        self.fc1 = nn.Linear(linear_channels, linear_channels*2)\n",
    "        self.fc2 = nn.Linear(linear_channels*2, linear_channels//2)\n",
    "\n",
    "        self.fc3 = nn.Linear(linear_channels, linear_channels*2)\n",
    "        self.fc4 = nn.Linear(linear_channels*2, linear_channels//2)\n",
    "        self.classify = nn.Linear(linear_channels, num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, adj, batch_size):\n",
    "        # 1. Obtain Conv features\n",
    "        x = x.view(x.shape[0]*x.shape[1], 1, x.shape[2], x.shape[3], x.shape[4])\n",
    "        x = F.relu(self.conv3d(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3d_2(x))\n",
    "        x = self.maxpool_2(x)\n",
    "        xs = x.shape  # Save the shape for reshaping later. format -> (batch_size * num_of_patch_in_each_image, 4, 4, 2)\n",
    "        x = x.view(x.shape[0]*x.shape[1], -1)\n",
    "        x = x.view(xs[0], -1)\n",
    "        \n",
    "        # 2. Obtain Diagonal Blocks. This is required for the GCNConv layer to work\n",
    "        block = adj[0]\n",
    "        for i in range(1, adj.shape[0]):\n",
    "            block = torch.block_diag(block, adj[i])\n",
    "        edge_index = block.to_sparse()._indices()\n",
    "        \n",
    "        # 2. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 3. Readout layer\n",
    "        x = x.view(batch_size, -1)\n",
    "        x_max = self.maxpool1d(x)\n",
    "        x_max = F.relu(self.fc1(x_max))\n",
    "        x_max = F.relu(self.fc2(x_max)) # [batch_size, linear_channels*2]  ---> [batch_size, linear_channels/2]\n",
    "\n",
    "        x_avg = self.avgpool1d(x)\n",
    "        x_avg = F.relu(self.fc3(x_avg))\n",
    "        x_avg = F.relu(self.fc4(x_avg))\n",
    "        x = torch.cat((x_max, x_avg), dim=1)\n",
    "        \n",
    "        #x = global_mean_pool(x, batch=torch.tensor([0, 1, 2, 3,  4, 5, 6, 7]).to(device='cuda'), size=batch_size)  # [batch_size, hidden_channels]\n",
    "        \n",
    "\n",
    "        # 4. Apply a final classifier\n",
    "        #x = F.dropout(x, p=0.1, training=self.training)\n",
    "\n",
    "        x = F.softmax(self.classify(x), dim=-1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02666965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the device and model\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GCN(num_node_features=256, \n",
    "                    num_classes=2,\n",
    "                    hidden_channels=32,\n",
    "                    linear_channels=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08ec9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv3d): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv3d_2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "  (maxpool_2): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv1): GCNConv(256, 32)\n",
      "  (conv2): GCNConv(32, 16)\n",
      "  (conv3): GCNConv(16, 8)\n",
      "  (maxpool1d): MaxPool1d(kernel_size=64, stride=64, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgpool1d): AvgPool1d(kernel_size=(64,), stride=(64,), padding=(0,))\n",
      "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (classify): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e08ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 122394, Trainable Parameters 122394\n",
      "Total parameters: 0.122394M, Trainable Parameters 0.122394M\n"
     ]
    }
   ],
   "source": [
    "# Get number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}, Trainable Parameters {total_trainable_params}\\nTotal parameters: {total_params/1000000}M, Trainable Parameters {total_trainable_params/1000000}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5614474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|     Modules      | Parameters |\n",
      "+------------------+------------+\n",
      "|  conv3d.weight   |    432     |\n",
      "|   conv3d.bias    |     16     |\n",
      "| conv3d_2.weight  |   13824    |\n",
      "|  conv3d_2.bias   |     32     |\n",
      "|    conv1.bias    |     32     |\n",
      "| conv1.lin.weight |    8192    |\n",
      "|    conv2.bias    |     16     |\n",
      "| conv2.lin.weight |    512     |\n",
      "|    conv3.bias    |     8      |\n",
      "| conv3.lin.weight |    128     |\n",
      "|    fc1.weight    |   32768    |\n",
      "|     fc1.bias     |    256     |\n",
      "|    fc2.weight    |   16384    |\n",
      "|     fc2.bias     |     64     |\n",
      "|    fc3.weight    |   32768    |\n",
      "|     fc3.bias     |    256     |\n",
      "|    fc4.weight    |   16384    |\n",
      "|     fc4.bias     |     64     |\n",
      "| classify.weight  |    256     |\n",
      "|  classify.bias   |     2      |\n",
      "+------------------+------------+\n",
      "Total Trainable Params: 122394\n"
     ]
    }
   ],
   "source": [
    "# Get parameters for each layer of the model in a tabular format\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0c4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=0.0005,\n",
    "            weight_decay=1e-5,\n",
    "            betas=(0.75, 0.999))\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.1)\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc22c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    total_time_iter = 0\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    train_loss, n_samples = 0, 0\n",
    "    correct = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        if labels.dim() == 3:\n",
    "            labels = torch.squeeze(labels)\n",
    "        patched_images, adj = build_graph(images)\n",
    "        patched_images, adj, labels = patched_images.to(device), adj.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(patched_images, adj, batch_size=images.shape[0])\n",
    "        loss = loss_fn(output, labels.to(dtype=torch.float32), reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        time_iter = time.time() - start\n",
    "        total_time_iter += time_iter\n",
    "        train_loss += loss.item() * len(output)\n",
    "        n_samples += len(output)\n",
    "        #print(output)\n",
    "        predicted, labels = torch.argmax(output, dim=1), torch.argmax(labels, dim=1)\n",
    "        #print(predicted, labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "        if batch_idx % 10 == 0 or batch_idx == len(train_loader) - 1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} (avg: {:.6f}) \\tsec/iter: {:.4f}%\\t Accuracy (avg) {:.3f}'.format(\n",
    "                epoch, n_samples, len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples, time_iter / (batch_idx + 1), 100*(correct/n_samples) ))\n",
    "    scheduler.step()\n",
    "    return total_time_iter / (batch_idx + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e2dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(valid_loader):\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    valid_loss, correct, n_samples = 0, 0, 0\n",
    "    for batch_idx, data in enumerate(valid_loader):\n",
    "        images, labels = data\n",
    "        if labels.dim() == 3:\n",
    "            labels = torch.squeeze(labels)\n",
    "        patched_images, adj = build_graph(images)\n",
    "        patched_images, adj, labels = patched_images.to(device), adj.to(device), labels.to(device)\n",
    "        output = model(patched_images, adj, batch_size=images.shape[0])\n",
    "        loss = loss_fn(output, labels.to(dtype=torch.float32), reduction='sum')\n",
    "        valid_loss += loss.item()\n",
    "        n_samples += len(output)\n",
    "        pred, labels = torch.argmax(output.data, dim=1), torch.argmax(labels, dim=1)\n",
    "\n",
    "        correct += (pred == labels).sum()\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    valid_loss /= n_samples\n",
    "\n",
    "    acc = 100. * correct / n_samples\n",
    "    print('Validation set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%) Took {} sec\\n'.format(epoch, \n",
    "                                                                                          valid_loss, \n",
    "                                                                                          correct, \n",
    "                                                                                          n_samples, acc, \n",
    "                                                                                          time_iter))\n",
    "    return valid_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9b6518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [8/565 (1%)]\tLoss: 0.315393 (avg: 0.315393) \tsec/iter: 1.9683%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 0 [88/565 (15%)]\tLoss: 0.316853 (avg: 0.366125) \tsec/iter: 2.0953%\t Accuracy (avg) 95.455\n",
      "Train Epoch: 0 [168/565 (30%)]\tLoss: 0.317323 (avg: 0.348203) \tsec/iter: 2.0996%\t Accuracy (avg) 97.024\n",
      "Train Epoch: 0 [248/565 (44%)]\tLoss: 0.324811 (avg: 0.348333) \tsec/iter: 2.1016%\t Accuracy (avg) 97.177\n",
      "Train Epoch: 0 [328/565 (58%)]\tLoss: 0.440198 (avg: 0.344324) \tsec/iter: 2.0978%\t Accuracy (avg) 97.561\n",
      "Train Epoch: 0 [408/565 (72%)]\tLoss: 0.314354 (avg: 0.349757) \tsec/iter: 2.0958%\t Accuracy (avg) 97.059\n",
      "Train Epoch: 0 [488/565 (86%)]\tLoss: 0.438950 (avg: 0.356889) \tsec/iter: 2.0948%\t Accuracy (avg) 96.107\n",
      "Train Epoch: 0 [565/565 (100%)]\tLoss: 0.332008 (avg: 0.357149) \tsec/iter: 2.0867%\t Accuracy (avg) 96.283\n",
      "Validation set (epoch 0): Average loss: 0.3926, Accuracy: 65/70 (92.86%) Took 18.370365142822266 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 1 [8/565 (1%)]\tLoss: 0.334343 (avg: 0.334343) \tsec/iter: 2.0761%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 1 [88/565 (15%)]\tLoss: 0.323496 (avg: 0.367572) \tsec/iter: 2.1550%\t Accuracy (avg) 95.455\n",
      "Train Epoch: 1 [168/565 (30%)]\tLoss: 0.313322 (avg: 0.369878) \tsec/iter: 2.1662%\t Accuracy (avg) 95.238\n",
      "Train Epoch: 1 [248/565 (44%)]\tLoss: 0.328699 (avg: 0.366389) \tsec/iter: 2.1734%\t Accuracy (avg) 95.161\n",
      "Train Epoch: 1 [328/565 (58%)]\tLoss: 0.313462 (avg: 0.365815) \tsec/iter: 2.1799%\t Accuracy (avg) 95.122\n",
      "Train Epoch: 1 [408/565 (72%)]\tLoss: 0.339740 (avg: 0.359885) \tsec/iter: 2.1751%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 1 [488/565 (86%)]\tLoss: 0.315271 (avg: 0.360520) \tsec/iter: 2.1724%\t Accuracy (avg) 95.902\n",
      "Train Epoch: 1 [565/565 (100%)]\tLoss: 0.313284 (avg: 0.356626) \tsec/iter: 2.1616%\t Accuracy (avg) 96.283\n",
      "Validation set (epoch 1): Average loss: 0.3924, Accuracy: 65/70 (92.86%) Took 18.533689737319946 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 2 [8/565 (1%)]\tLoss: 0.438927 (avg: 0.438927) \tsec/iter: 2.0706%\t Accuracy (avg) 87.500\n",
      "Train Epoch: 2 [88/565 (15%)]\tLoss: 0.323417 (avg: 0.345874) \tsec/iter: 2.1561%\t Accuracy (avg) 97.727\n",
      "Train Epoch: 2 [168/565 (30%)]\tLoss: 0.317484 (avg: 0.339378) \tsec/iter: 2.1571%\t Accuracy (avg) 98.214\n",
      "Train Epoch: 2 [248/565 (44%)]\tLoss: 0.343269 (avg: 0.348630) \tsec/iter: 2.1497%\t Accuracy (avg) 97.581\n",
      "Train Epoch: 2 [328/565 (58%)]\tLoss: 0.439054 (avg: 0.351259) \tsec/iter: 2.1431%\t Accuracy (avg) 97.561\n",
      "Train Epoch: 2 [408/565 (72%)]\tLoss: 0.313289 (avg: 0.348629) \tsec/iter: 2.1422%\t Accuracy (avg) 97.549\n",
      "Train Epoch: 2 [488/565 (86%)]\tLoss: 0.448433 (avg: 0.351814) \tsec/iter: 2.1415%\t Accuracy (avg) 97.131\n",
      "Train Epoch: 2 [565/565 (100%)]\tLoss: 0.383681 (avg: 0.356454) \tsec/iter: 2.1272%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 2): Average loss: 0.3924, Accuracy: 64/70 (91.43%) Took 18.1994731426239 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 3 [8/565 (1%)]\tLoss: 0.354759 (avg: 0.354759) \tsec/iter: 2.0324%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 3 [88/565 (15%)]\tLoss: 0.324914 (avg: 0.377015) \tsec/iter: 2.0977%\t Accuracy (avg) 95.455\n",
      "Train Epoch: 3 [168/565 (30%)]\tLoss: 0.447024 (avg: 0.365663) \tsec/iter: 2.1271%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 3 [248/565 (44%)]\tLoss: 0.535496 (avg: 0.366938) \tsec/iter: 2.1333%\t Accuracy (avg) 95.565\n",
      "Train Epoch: 3 [328/565 (58%)]\tLoss: 0.429588 (avg: 0.361935) \tsec/iter: 2.1440%\t Accuracy (avg) 95.732\n",
      "Train Epoch: 3 [408/565 (72%)]\tLoss: 0.469870 (avg: 0.357106) \tsec/iter: 2.1474%\t Accuracy (avg) 96.324\n",
      "Train Epoch: 3 [488/565 (86%)]\tLoss: 0.439828 (avg: 0.355513) \tsec/iter: 2.1432%\t Accuracy (avg) 96.516\n",
      "Train Epoch: 3 [565/565 (100%)]\tLoss: 0.319286 (avg: 0.356657) \tsec/iter: 2.1339%\t Accuracy (avg) 96.460\n",
      "Validation set (epoch 3): Average loss: 0.3923, Accuracy: 64/70 (91.43%) Took 18.014457941055298 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 4 [8/565 (1%)]\tLoss: 0.313979 (avg: 0.313979) \tsec/iter: 2.0279%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 4 [88/565 (15%)]\tLoss: 0.315932 (avg: 0.368252) \tsec/iter: 2.0683%\t Accuracy (avg) 95.455\n",
      "Train Epoch: 4 [168/565 (30%)]\tLoss: 0.335384 (avg: 0.365381) \tsec/iter: 2.0561%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 4 [248/565 (44%)]\tLoss: 0.313364 (avg: 0.360805) \tsec/iter: 2.0606%\t Accuracy (avg) 95.968\n",
      "Train Epoch: 4 [328/565 (58%)]\tLoss: 0.600875 (avg: 0.363081) \tsec/iter: 2.0666%\t Accuracy (avg) 96.037\n",
      "Train Epoch: 4 [408/565 (72%)]\tLoss: 0.316874 (avg: 0.365488) \tsec/iter: 2.0701%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 4 [488/565 (86%)]\tLoss: 0.319707 (avg: 0.359052) \tsec/iter: 2.0718%\t Accuracy (avg) 96.311\n",
      "Train Epoch: 4 [565/565 (100%)]\tLoss: 0.313343 (avg: 0.356371) \tsec/iter: 2.0601%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 4): Average loss: 0.3922, Accuracy: 64/70 (91.43%) Took 18.20318913459778 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 5 [8/565 (1%)]\tLoss: 0.315994 (avg: 0.315994) \tsec/iter: 2.0243%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 5 [88/565 (15%)]\tLoss: 0.448898 (avg: 0.342474) \tsec/iter: 2.0725%\t Accuracy (avg) 97.727\n",
      "Train Epoch: 5 [168/565 (30%)]\tLoss: 0.317950 (avg: 0.351234) \tsec/iter: 2.0719%\t Accuracy (avg) 96.429\n",
      "Train Epoch: 5 [248/565 (44%)]\tLoss: 0.318816 (avg: 0.359416) \tsec/iter: 2.0758%\t Accuracy (avg) 95.968\n",
      "Train Epoch: 5 [328/565 (58%)]\tLoss: 0.444500 (avg: 0.355828) \tsec/iter: 2.0810%\t Accuracy (avg) 96.341\n",
      "Train Epoch: 5 [408/565 (72%)]\tLoss: 0.352335 (avg: 0.354349) \tsec/iter: 2.0844%\t Accuracy (avg) 96.569\n",
      "Train Epoch: 5 [488/565 (86%)]\tLoss: 0.315878 (avg: 0.359585) \tsec/iter: 2.0929%\t Accuracy (avg) 96.107\n",
      "Train Epoch: 5 [565/565 (100%)]\tLoss: 0.314117 (avg: 0.356197) \tsec/iter: 2.0808%\t Accuracy (avg) 96.460\n",
      "Validation set (epoch 5): Average loss: 0.3920, Accuracy: 65/70 (92.86%) Took 17.514727115631104 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 6 [8/565 (1%)]\tLoss: 0.318915 (avg: 0.318915) \tsec/iter: 2.0629%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 6 [88/565 (15%)]\tLoss: 0.313331 (avg: 0.323469) \tsec/iter: 2.0837%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 6 [168/565 (30%)]\tLoss: 0.426894 (avg: 0.342779) \tsec/iter: 2.0802%\t Accuracy (avg) 97.619\n",
      "Train Epoch: 6 [248/565 (44%)]\tLoss: 0.437915 (avg: 0.348634) \tsec/iter: 2.0821%\t Accuracy (avg) 96.774\n",
      "Train Epoch: 6 [328/565 (58%)]\tLoss: 0.346301 (avg: 0.350757) \tsec/iter: 2.0811%\t Accuracy (avg) 96.646\n",
      "Train Epoch: 6 [408/565 (72%)]\tLoss: 0.326609 (avg: 0.351866) \tsec/iter: 2.0803%\t Accuracy (avg) 96.569\n",
      "Train Epoch: 6 [488/565 (86%)]\tLoss: 0.470077 (avg: 0.357363) \tsec/iter: 2.0792%\t Accuracy (avg) 96.107\n",
      "Train Epoch: 6 [565/565 (100%)]\tLoss: 0.361175 (avg: 0.356236) \tsec/iter: 2.0657%\t Accuracy (avg) 96.283\n",
      "Validation set (epoch 6): Average loss: 0.3920, Accuracy: 65/70 (92.86%) Took 17.827833890914917 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 7 [8/565 (1%)]\tLoss: 0.469214 (avg: 0.469214) \tsec/iter: 2.1200%\t Accuracy (avg) 87.500\n",
      "Train Epoch: 7 [88/565 (15%)]\tLoss: 0.330143 (avg: 0.363533) \tsec/iter: 2.0680%\t Accuracy (avg) 96.591\n",
      "Train Epoch: 7 [168/565 (30%)]\tLoss: 0.316941 (avg: 0.354569) \tsec/iter: 2.0677%\t Accuracy (avg) 97.024\n",
      "Train Epoch: 7 [248/565 (44%)]\tLoss: 0.331957 (avg: 0.359530) \tsec/iter: 2.0721%\t Accuracy (avg) 96.371\n",
      "Train Epoch: 7 [328/565 (58%)]\tLoss: 0.355061 (avg: 0.356035) \tsec/iter: 2.0798%\t Accuracy (avg) 96.646\n",
      "Train Epoch: 7 [408/565 (72%)]\tLoss: 0.441408 (avg: 0.358747) \tsec/iter: 2.0790%\t Accuracy (avg) 96.324\n",
      "Train Epoch: 7 [488/565 (86%)]\tLoss: 0.416467 (avg: 0.356698) \tsec/iter: 2.0808%\t Accuracy (avg) 96.311\n",
      "Train Epoch: 7 [565/565 (100%)]\tLoss: 0.358882 (avg: 0.356110) \tsec/iter: 2.0703%\t Accuracy (avg) 96.460\n",
      "Validation set (epoch 7): Average loss: 0.3921, Accuracy: 64/70 (91.43%) Took 17.62971043586731 sec\n",
      "\n",
      "Train Epoch: 8 [8/565 (1%)]\tLoss: 0.353080 (avg: 0.353080) \tsec/iter: 1.9752%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 8 [88/565 (15%)]\tLoss: 0.314792 (avg: 0.335930) \tsec/iter: 2.0565%\t Accuracy (avg) 98.864\n",
      "Train Epoch: 8 [168/565 (30%)]\tLoss: 0.324438 (avg: 0.337002) \tsec/iter: 2.0768%\t Accuracy (avg) 98.810\n",
      "Train Epoch: 8 [248/565 (44%)]\tLoss: 0.338691 (avg: 0.340326) \tsec/iter: 2.0758%\t Accuracy (avg) 98.387\n",
      "Train Epoch: 8 [328/565 (58%)]\tLoss: 0.547923 (avg: 0.352221) \tsec/iter: 2.0751%\t Accuracy (avg) 96.951\n",
      "Train Epoch: 8 [408/565 (72%)]\tLoss: 0.327433 (avg: 0.358899) \tsec/iter: 2.0808%\t Accuracy (avg) 96.324\n",
      "Train Epoch: 8 [488/565 (86%)]\tLoss: 0.323230 (avg: 0.352773) \tsec/iter: 2.0853%\t Accuracy (avg) 96.926\n",
      "Train Epoch: 8 [565/565 (100%)]\tLoss: 0.347823 (avg: 0.355993) \tsec/iter: 2.0748%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 8): Average loss: 0.3919, Accuracy: 65/70 (92.86%) Took 17.685707092285156 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 9 [8/565 (1%)]\tLoss: 0.314836 (avg: 0.314836) \tsec/iter: 2.0231%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 9 [88/565 (15%)]\tLoss: 0.350779 (avg: 0.378286) \tsec/iter: 2.1025%\t Accuracy (avg) 93.182\n",
      "Train Epoch: 9 [168/565 (30%)]\tLoss: 0.314113 (avg: 0.365871) \tsec/iter: 2.1100%\t Accuracy (avg) 95.238\n",
      "Train Epoch: 9 [248/565 (44%)]\tLoss: 0.316341 (avg: 0.354823) \tsec/iter: 2.1144%\t Accuracy (avg) 96.371\n",
      "Train Epoch: 9 [328/565 (58%)]\tLoss: 0.315762 (avg: 0.353511) \tsec/iter: 2.1168%\t Accuracy (avg) 96.646\n",
      "Train Epoch: 9 [408/565 (72%)]\tLoss: 0.313324 (avg: 0.360900) \tsec/iter: 2.1177%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 9 [488/565 (86%)]\tLoss: 0.315219 (avg: 0.358583) \tsec/iter: 2.1176%\t Accuracy (avg) 96.107\n",
      "Train Epoch: 9 [565/565 (100%)]\tLoss: 0.319814 (avg: 0.356040) \tsec/iter: 2.1042%\t Accuracy (avg) 96.460\n",
      "Validation set (epoch 9): Average loss: 0.3918, Accuracy: 65/70 (92.86%) Took 17.446274042129517 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 10 [8/565 (1%)]\tLoss: 0.314363 (avg: 0.314363) \tsec/iter: 1.9723%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 10 [88/565 (15%)]\tLoss: 0.441149 (avg: 0.351714) \tsec/iter: 2.0588%\t Accuracy (avg) 96.591\n",
      "Train Epoch: 10 [168/565 (30%)]\tLoss: 0.319694 (avg: 0.357306) \tsec/iter: 2.0625%\t Accuracy (avg) 96.429\n",
      "Train Epoch: 10 [248/565 (44%)]\tLoss: 0.317376 (avg: 0.356554) \tsec/iter: 2.0663%\t Accuracy (avg) 96.371\n",
      "Train Epoch: 10 [328/565 (58%)]\tLoss: 0.448248 (avg: 0.357399) \tsec/iter: 2.0691%\t Accuracy (avg) 96.341\n",
      "Train Epoch: 10 [408/565 (72%)]\tLoss: 0.530254 (avg: 0.360847) \tsec/iter: 2.0706%\t Accuracy (avg) 96.078\n",
      "Train Epoch: 10 [488/565 (86%)]\tLoss: 0.349047 (avg: 0.356518) \tsec/iter: 2.0692%\t Accuracy (avg) 96.516\n",
      "Train Epoch: 10 [565/565 (100%)]\tLoss: 0.330982 (avg: 0.355974) \tsec/iter: 2.0594%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 10): Average loss: 0.3918, Accuracy: 65/70 (92.86%) Took 17.693203449249268 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 11 [8/565 (1%)]\tLoss: 0.316441 (avg: 0.316441) \tsec/iter: 1.9967%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 11 [88/565 (15%)]\tLoss: 0.327263 (avg: 0.341243) \tsec/iter: 2.0932%\t Accuracy (avg) 97.727\n",
      "Train Epoch: 11 [168/565 (30%)]\tLoss: 0.334635 (avg: 0.361363) \tsec/iter: 2.1254%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 11 [248/565 (44%)]\tLoss: 0.313307 (avg: 0.356699) \tsec/iter: 2.1114%\t Accuracy (avg) 96.371\n",
      "Train Epoch: 11 [328/565 (58%)]\tLoss: 0.365087 (avg: 0.360380) \tsec/iter: 2.1071%\t Accuracy (avg) 96.037\n",
      "Train Epoch: 11 [408/565 (72%)]\tLoss: 0.445507 (avg: 0.360586) \tsec/iter: 2.1022%\t Accuracy (avg) 96.078\n",
      "Train Epoch: 11 [488/565 (86%)]\tLoss: 0.437818 (avg: 0.355994) \tsec/iter: 2.0949%\t Accuracy (avg) 96.516\n",
      "Train Epoch: 11 [565/565 (100%)]\tLoss: 0.317277 (avg: 0.355802) \tsec/iter: 2.0798%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 11): Average loss: 0.3918, Accuracy: 65/70 (92.86%) Took 17.65278196334839 sec\n",
      "\n",
      "Train Epoch: 12 [8/565 (1%)]\tLoss: 0.427141 (avg: 0.427141) \tsec/iter: 2.0444%\t Accuracy (avg) 87.500\n",
      "Train Epoch: 12 [88/565 (15%)]\tLoss: 0.560877 (avg: 0.404758) \tsec/iter: 2.1014%\t Accuracy (avg) 92.045\n",
      "Train Epoch: 12 [168/565 (30%)]\tLoss: 0.321097 (avg: 0.366350) \tsec/iter: 2.0899%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 12 [248/565 (44%)]\tLoss: 0.561939 (avg: 0.362354) \tsec/iter: 2.0829%\t Accuracy (avg) 95.968\n",
      "Train Epoch: 12 [328/565 (58%)]\tLoss: 0.438786 (avg: 0.367442) \tsec/iter: 2.0773%\t Accuracy (avg) 95.427\n",
      "Train Epoch: 12 [408/565 (72%)]\tLoss: 0.449671 (avg: 0.361684) \tsec/iter: 2.0739%\t Accuracy (avg) 96.078\n",
      "Train Epoch: 12 [488/565 (86%)]\tLoss: 0.347597 (avg: 0.357976) \tsec/iter: 2.0707%\t Accuracy (avg) 96.516\n",
      "Train Epoch: 12 [565/565 (100%)]\tLoss: 0.313721 (avg: 0.355873) \tsec/iter: 2.0598%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 12): Average loss: 0.3918, Accuracy: 65/70 (92.86%) Took 17.798887729644775 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 13 [8/565 (1%)]\tLoss: 0.436244 (avg: 0.436244) \tsec/iter: 2.0862%\t Accuracy (avg) 87.500\n",
      "Train Epoch: 13 [88/565 (15%)]\tLoss: 0.313576 (avg: 0.333805) \tsec/iter: 2.0620%\t Accuracy (avg) 98.864\n",
      "Train Epoch: 13 [168/565 (30%)]\tLoss: 0.325376 (avg: 0.342876) \tsec/iter: 2.0632%\t Accuracy (avg) 98.214\n",
      "Train Epoch: 13 [248/565 (44%)]\tLoss: 0.315223 (avg: 0.345403) \tsec/iter: 2.0735%\t Accuracy (avg) 97.984\n",
      "Train Epoch: 13 [328/565 (58%)]\tLoss: 0.316074 (avg: 0.347861) \tsec/iter: 2.0716%\t Accuracy (avg) 97.561\n",
      "Train Epoch: 13 [408/565 (72%)]\tLoss: 0.438816 (avg: 0.350034) \tsec/iter: 2.0668%\t Accuracy (avg) 97.304\n",
      "Train Epoch: 13 [488/565 (86%)]\tLoss: 0.443372 (avg: 0.356554) \tsec/iter: 2.0640%\t Accuracy (avg) 96.516\n",
      "Train Epoch: 13 [565/565 (100%)]\tLoss: 0.314625 (avg: 0.355662) \tsec/iter: 2.0500%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 13): Average loss: 0.3918, Accuracy: 65/70 (92.86%) Took 17.809367656707764 sec\n",
      "\n",
      "Train Epoch: 14 [8/565 (1%)]\tLoss: 0.453074 (avg: 0.453074) \tsec/iter: 1.9901%\t Accuracy (avg) 87.500\n",
      "Train Epoch: 14 [88/565 (15%)]\tLoss: 0.322724 (avg: 0.369591) \tsec/iter: 2.0747%\t Accuracy (avg) 94.318\n",
      "Train Epoch: 14 [168/565 (30%)]\tLoss: 0.440038 (avg: 0.358950) \tsec/iter: 2.0779%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 14 [248/565 (44%)]\tLoss: 0.463806 (avg: 0.352210) \tsec/iter: 2.0704%\t Accuracy (avg) 96.774\n",
      "Train Epoch: 14 [328/565 (58%)]\tLoss: 0.467407 (avg: 0.353773) \tsec/iter: 2.0702%\t Accuracy (avg) 96.951\n",
      "Train Epoch: 14 [408/565 (72%)]\tLoss: 0.355413 (avg: 0.351984) \tsec/iter: 2.0691%\t Accuracy (avg) 97.059\n",
      "Train Epoch: 14 [488/565 (86%)]\tLoss: 0.443731 (avg: 0.353726) \tsec/iter: 2.0678%\t Accuracy (avg) 96.926\n",
      "Train Epoch: 14 [565/565 (100%)]\tLoss: 0.513855 (avg: 0.355671) \tsec/iter: 2.0563%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 14): Average loss: 0.3918, Accuracy: 65/70 (92.86%) Took 18.02772092819214 sec\n",
      "\n",
      "Train Epoch: 15 [8/565 (1%)]\tLoss: 0.438535 (avg: 0.438535) \tsec/iter: 2.0016%\t Accuracy (avg) 87.500\n",
      "Train Epoch: 15 [88/565 (15%)]\tLoss: 0.313375 (avg: 0.377372) \tsec/iter: 2.0899%\t Accuracy (avg) 94.318\n",
      "Train Epoch: 15 [168/565 (30%)]\tLoss: 0.321538 (avg: 0.354121) \tsec/iter: 2.0835%\t Accuracy (avg) 96.429\n",
      "Train Epoch: 15 [248/565 (44%)]\tLoss: 0.313343 (avg: 0.350890) \tsec/iter: 2.0709%\t Accuracy (avg) 96.774\n",
      "Train Epoch: 15 [328/565 (58%)]\tLoss: 0.438260 (avg: 0.352785) \tsec/iter: 2.0662%\t Accuracy (avg) 96.646\n",
      "Train Epoch: 15 [408/565 (72%)]\tLoss: 0.326623 (avg: 0.355581) \tsec/iter: 2.0621%\t Accuracy (avg) 96.324\n",
      "Train Epoch: 15 [488/565 (86%)]\tLoss: 0.465456 (avg: 0.356345) \tsec/iter: 2.0604%\t Accuracy (avg) 96.516\n",
      "Train Epoch: 15 [565/565 (100%)]\tLoss: 0.554782 (avg: 0.355733) \tsec/iter: 2.0504%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 15): Average loss: 0.3917, Accuracy: 65/70 (92.86%) Took 17.586722373962402 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 16 [8/565 (1%)]\tLoss: 0.349068 (avg: 0.349068) \tsec/iter: 1.9894%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 16 [88/565 (15%)]\tLoss: 0.473501 (avg: 0.338288) \tsec/iter: 2.0587%\t Accuracy (avg) 98.864\n",
      "Train Epoch: 16 [168/565 (30%)]\tLoss: 0.315857 (avg: 0.337542) \tsec/iter: 2.0682%\t Accuracy (avg) 98.810\n",
      "Train Epoch: 16 [248/565 (44%)]\tLoss: 0.314313 (avg: 0.350510) \tsec/iter: 2.0554%\t Accuracy (avg) 97.177\n",
      "Train Epoch: 16 [328/565 (58%)]\tLoss: 0.409411 (avg: 0.364999) \tsec/iter: 2.0469%\t Accuracy (avg) 95.427\n",
      "Train Epoch: 16 [408/565 (72%)]\tLoss: 0.313265 (avg: 0.358929) \tsec/iter: 2.0478%\t Accuracy (avg) 96.078\n",
      "Train Epoch: 16 [488/565 (86%)]\tLoss: 0.359076 (avg: 0.357023) \tsec/iter: 2.0526%\t Accuracy (avg) 96.516\n",
      "Train Epoch: 16 [565/565 (100%)]\tLoss: 0.324872 (avg: 0.355416) \tsec/iter: 2.0427%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 16): Average loss: 0.3918, Accuracy: 65/70 (92.86%) Took 17.545891284942627 sec\n",
      "\n",
      "Train Epoch: 17 [8/565 (1%)]\tLoss: 0.475725 (avg: 0.475725) \tsec/iter: 1.9994%\t Accuracy (avg) 87.500\n",
      "Train Epoch: 17 [88/565 (15%)]\tLoss: 0.313328 (avg: 0.359209) \tsec/iter: 2.0845%\t Accuracy (avg) 96.591\n",
      "Train Epoch: 17 [168/565 (30%)]\tLoss: 0.317898 (avg: 0.363808) \tsec/iter: 2.0826%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 17 [248/565 (44%)]\tLoss: 0.317408 (avg: 0.351444) \tsec/iter: 2.0754%\t Accuracy (avg) 97.177\n",
      "Train Epoch: 17 [328/565 (58%)]\tLoss: 0.313513 (avg: 0.352781) \tsec/iter: 2.0662%\t Accuracy (avg) 96.951\n",
      "Train Epoch: 17 [408/565 (72%)]\tLoss: 0.559721 (avg: 0.355339) \tsec/iter: 2.0596%\t Accuracy (avg) 96.569\n",
      "Train Epoch: 17 [488/565 (86%)]\tLoss: 0.343256 (avg: 0.356464) \tsec/iter: 2.0568%\t Accuracy (avg) 96.516\n",
      "Train Epoch: 17 [565/565 (100%)]\tLoss: 0.313367 (avg: 0.355433) \tsec/iter: 2.0424%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 17): Average loss: 0.3917, Accuracy: 65/70 (92.86%) Took 17.334877729415894 sec\n",
      "\n",
      "Train Epoch: 18 [8/565 (1%)]\tLoss: 0.313675 (avg: 0.313675) \tsec/iter: 1.9558%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 18 [88/565 (15%)]\tLoss: 0.321459 (avg: 0.364691) \tsec/iter: 2.0582%\t Accuracy (avg) 95.455\n",
      "Train Epoch: 18 [168/565 (30%)]\tLoss: 0.313283 (avg: 0.358777) \tsec/iter: 2.0661%\t Accuracy (avg) 95.833\n",
      "Train Epoch: 18 [248/565 (44%)]\tLoss: 0.313263 (avg: 0.353245) \tsec/iter: 2.0598%\t Accuracy (avg) 96.371\n",
      "Train Epoch: 18 [328/565 (58%)]\tLoss: 0.432178 (avg: 0.351116) \tsec/iter: 2.0571%\t Accuracy (avg) 96.646\n",
      "Train Epoch: 18 [408/565 (72%)]\tLoss: 0.505150 (avg: 0.355534) \tsec/iter: 2.0566%\t Accuracy (avg) 96.569\n",
      "Train Epoch: 18 [488/565 (86%)]\tLoss: 0.326010 (avg: 0.358520) \tsec/iter: 2.0564%\t Accuracy (avg) 96.311\n",
      "Train Epoch: 18 [565/565 (100%)]\tLoss: 0.313399 (avg: 0.355456) \tsec/iter: 2.0417%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 18): Average loss: 0.3917, Accuracy: 65/70 (92.86%) Took 17.203919887542725 sec\n",
      "\n",
      "Model Saved\n",
      "Train Epoch: 19 [8/565 (1%)]\tLoss: 0.323624 (avg: 0.323624) \tsec/iter: 1.9672%\t Accuracy (avg) 100.000\n",
      "Train Epoch: 19 [88/565 (15%)]\tLoss: 0.316287 (avg: 0.346017) \tsec/iter: 2.0198%\t Accuracy (avg) 97.727\n",
      "Train Epoch: 19 [168/565 (30%)]\tLoss: 0.313269 (avg: 0.353568) \tsec/iter: 2.0393%\t Accuracy (avg) 97.024\n",
      "Train Epoch: 19 [248/565 (44%)]\tLoss: 0.322033 (avg: 0.357106) \tsec/iter: 2.0550%\t Accuracy (avg) 96.774\n",
      "Train Epoch: 19 [328/565 (58%)]\tLoss: 0.333432 (avg: 0.355227) \tsec/iter: 2.0453%\t Accuracy (avg) 96.646\n",
      "Train Epoch: 19 [408/565 (72%)]\tLoss: 0.328444 (avg: 0.354817) \tsec/iter: 2.0396%\t Accuracy (avg) 96.569\n",
      "Train Epoch: 19 [488/565 (86%)]\tLoss: 0.322191 (avg: 0.356152) \tsec/iter: 2.0394%\t Accuracy (avg) 96.516\n",
      "Train Epoch: 19 [565/565 (100%)]\tLoss: 0.315737 (avg: 0.355283) \tsec/iter: 2.0275%\t Accuracy (avg) 96.637\n",
      "Validation set (epoch 19): Average loss: 0.3917, Accuracy: 65/70 (92.86%) Took 17.3627827167511 sec\n",
      "\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "epochs=20 # Number of epochs --> 30+20 = 50\n",
    "# Train the model. Save the model with the best validation accuracy. Only last epoch execution is shown\n",
    "best_loss = 100000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader)\n",
    "    valid_loss, acc = validation(valid_loader)\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), os.getcwd() + f'/best_model{datetime.now()}.pt')\n",
    "        print('Model Saved')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e660d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader):\n",
    "    print('Test model ...')\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    test_loss, correct, n_samples = 0, 0, 0\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        images, labels = data\n",
    "        patched_images, adj = build_graph(images)\n",
    "        patched_images, adj, labels = patched_images.to(device), adj.to(device), labels.to(device)\n",
    "        if labels.dim() == 3:\n",
    "            labels = torch.squeeze(labels)\n",
    "        output = model(patched_images, adj, batch_size=images.shape[0])\n",
    "        loss = loss_fn(output, labels.to(device=device, dtype=torch.float32), reduction='sum')\n",
    "        test_loss += loss.item()\n",
    "        n_samples += len(output)\n",
    "        pred = torch.argmax(output.data, 1)\n",
    "        print(pred)\n",
    "\n",
    "        correct += (pred == torch.argmax(labels, dim=1)).sum()\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    test_loss /= n_samples\n",
    "\n",
    "    acc = 100. * correct / n_samples\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%). Took {} sec time'.format(test_loss, \n",
    "                                                                                correct, \n",
    "                                                                                n_samples, acc, time_iter))\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5293b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test model ...\n",
      "Test set: Average loss: 0.3908, Accuracy: 67/72 (93.06%). Took 17.238627910614014 sec time\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "loaded_model = GCN(num_node_features=256,\n",
    "                    num_classes=2,\n",
    "                    hidden_channels=32,\n",
    "                    linear_channels=128).to(device)\n",
    "loaded_model.load_state_dict(torch.load(\"best_model2023-09-22 16:11:28.118115.pt\"))\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "def loaded_test(test_loader):\n",
    "    print('Test model ...')\n",
    "    loaded_model.eval()\n",
    "    start = time.time()\n",
    "    test_loss, correct, n_samples = 0, 0, 0\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        images, labels = data\n",
    "        patched_images, adj = build_graph(images)\n",
    "        patched_images, adj, labels = patched_images.to(device), adj.to(device), labels.to(device)\n",
    "        if labels.dim() == 3:\n",
    "            labels = torch.squeeze(labels)\n",
    "        output = loaded_model(patched_images, adj, batch_size=images.shape[0])\n",
    "        loss = loss_fn(output, labels.to(device=device, dtype=torch.float32), reduction='sum')\n",
    "        test_loss += loss.item()\n",
    "        n_samples += len(output)\n",
    "        pred = torch.argmax(output.data, 1)\n",
    "        #print(pred, torch.argmax(labels, dim=1))\n",
    "\n",
    "        correct += (pred == torch.argmax(labels, dim=1)).sum()\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    test_loss /= n_samples\n",
    "\n",
    "    acc = 100. * correct / n_samples\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%). Took {} sec time'.format(test_loss, \n",
    "                                                                                correct, \n",
    "                                                                                n_samples, acc, time_iter))\n",
    "    return test_loss, acc\n",
    "\n",
    "# Test the model\n",
    "test_loss, acc = loaded_test(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
